{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and loading data in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#____loading data____\n",
    "# df = pd.read_csv('./data/clean_house.csv')\n",
    "df = pd.read_csv('./data/clean_app.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA summary\n",
    "\n",
    "- we have a dataframe with 25 columns\n",
    "- 8 columns have categorical data:\n",
    "    - locality_name\n",
    "    - main_city\n",
    "    - province\n",
    "    - property_type\n",
    "    - type_of_sale\n",
    "    - kitchen_type\n",
    "    - state_of_building\n",
    "    - property_subtype\n",
    "\n",
    "- for houses we have 11394 rows\n",
    "\n",
    "- These are the columns that have null values:\n",
    "\n",
    "```\n",
    "    number_of_rooms           8188\n",
    "    terrace_area              7969\n",
    "    swimming_pool             7550\n",
    "    garden_area               7511\n",
    "    furnished                 7404\n",
    "    garden                    6775\n",
    "    terrace                   4813\n",
    "    kitchen_type              4046\n",
    "    fully_equipped_kitchen    2422\n",
    "    state_of_building         2338\n",
    "    number_of_facades         1765\n",
    "    latitude                  1468\n",
    "    longitude                 1468\n",
    "```\n",
    "\n",
    "### REMARKS\n",
    "\n",
    "- columns that should be converted to int (not sure if needed): `number of rooms`, `living area`, `fully equipped kitchen`, `furnished`, `terrace`, `garden`, `garden area`, `surface of good`, `number of facades`, `swimming pool`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ____Outliers handling_____\n",
    "import numpy as np \n",
    "\n",
    "Q1 = df[\"price\"].quantile(0.25)\n",
    "Q3 = df[\"price\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "max_value = Q3 + (1.5 * IQR)\n",
    "min_value = Q1 - (1.5 * IQR)\n",
    "\n",
    "outliers_mask = (df[\"price\"] < min_value) | (df[\"price\"] > max_value)\n",
    "df.loc[outliers_mask, \"price\"] = np.nan\n",
    "\n",
    "df.dropna(subset=[\"price\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing\n",
    "\n",
    "## converting dtypes\n",
    "\n",
    "- columns that should be converted to int: `number of rooms`, `living area`, `fully equipped kitchen`, `furnished`, `terrace`, `garden`, `garden area`, `surface of good`, `number of facades`, `swimming pool`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ____converting dtypes____\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# List of columns to convert to integer\n",
    "columns_to_convert = ['number_of_rooms', 'living_area', 'fully_equipped_kitchen', 'furnished', \n",
    "                      'terrace', 'terrace_area', 'garden', 'garden_area', 'surface_of_good', \n",
    "                      'number_of_facades', 'swimming_pool']\n",
    "\n",
    "# Convert specified columns to integer type\n",
    "df[columns_to_convert] = df[columns_to_convert].apply(pd.to_numeric, errors='coerce').fillna(pd.NA)\n",
    "\n",
    "df[columns_to_convert] = df[columns_to_convert].astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop unnecessary columns\n",
    "\n",
    "- `property_id` uniquely identifies a property (therefor it is not very useful for predicting a given outcome)\n",
    "- same for `latitude`/`longitude` --> they were handy for the visuals, but have no added value for training a model\n",
    "- `property_type` is not useful for predicting a given outcome as well as the split of houses and appartments was already done upfront in the cleaning resulting in different files\n",
    "- `type_of_sale` is not useful for predicting a given outcome as most properties have a \"BUY REGULAR\" as value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _____dropping unnecessary columns____\n",
    "# for house and app\n",
    "columns_to_drop = ['property_id', 'latitude', 'longitude', 'property_type', 'type_of_sale', 'fully_equipped_kitchen']\n",
    "\n",
    "df.drop(columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for app only\n",
    "\n",
    "app_columns_to_drop = ['surface_of_good']\n",
    "\n",
    "df.drop(app_columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing values handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list only columns that have null values\n",
    "null_colums = df.columns[df.isnull().any()]\n",
    "print(df[null_colums].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling up swimming pool column with 0 when value is not filled in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _____imputing missing values for swimmingpool____\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "constant_imputer = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "df[['swimming_pool']] = constant_imputer.fit_transform(df[['swimming_pool']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding\n",
    "\n",
    "- To handle columns:\n",
    "\n",
    "- kitchen_type              4046 --> object      -----> One Hot Encoding\n",
    "- state_of_building         2338 --> object      -----> One Hot Encoding\n",
    "- property_subtype          0                    -----> One Hot Encoding    \n",
    "\n",
    "    ```\n",
    "    number_of_rooms           8188\n",
    "    terrace_area              7969\n",
    "    garden_area               7511\n",
    "    furnished                 7404 --> 1/0 or NaN\n",
    "    garden                    6775 --> 1/0 or NaN\n",
    "    terrace                   4813 --> 1/0 or NaN\n",
    "    fully_equipped_kitchen    2422 --> 1/0 or NaN\n",
    "    number_of_facades         1765\n",
    "    ```\n",
    "\n",
    "`OneHotEncoder` from the `sklearn.preprocessing` module and `pd.get_dummies` from pandas are both used for one-hot encoding categorical variables.\n",
    "\n",
    "The main differences are:\n",
    "\n",
    "- `OneHotEncoder` is typically used in a machine learning pipeline within scikit-learn, while `pd.get_dummies` is a pandas function.\n",
    "- `OneHotEncoder` requires you to first fit the encoder on the data and then transform it, while `pd.get_dummies` can be applied directly to the DataFrame.\n",
    "- `OneHotEncoder` returns a sparse matrix by default, which can be memory efficient for large datasets, while `pd.get_dummies` returns a pandas DataFrame.\n",
    "- `OneHotEncoder` can handle new categories not seen during fitting better than `pd.get_dummies`.\n",
    "- Depending on your workflow and requirements, you can choose between the two options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _____one-hot encoding for kitchen_type____\n",
    "df = pd.get_dummies(df, columns=[\"kitchen_type\"], prefix=\"kitchen_type\")\n",
    "\n",
    "# _____one-hot encoding for state_of_building____\n",
    "df = pd.get_dummies(df, columns=[\"state_of_building\"], prefix=\"state_of_building\")\n",
    "\n",
    "# _____one-hot encoding for property_subtype____\n",
    "df = pd.get_dummies(df, columns=[\"property_subtype\"], prefix=\"property_subtype\")\n",
    "\n",
    "# _____one-hot encoding for province____\n",
    "df = pd.get_dummies(df, columns=[\"province\"], prefix=\"province\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list only columns that have null values\n",
    "null_colums = df.columns[df.isnull().any()]\n",
    "print(df[null_colums].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "- Converting string columns to number\n",
    "\n",
    "```python\n",
    "# _____feature engineering____\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit_transform(df['type_of_sale'])[0:10]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a training and test set\n",
    "\n",
    "\n",
    "#### Stratified sampling\n",
    "\n",
    "- Can be used for when y is more classification values\n",
    "    `X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)`\n",
    "\n",
    "#### About datatypes\n",
    "\n",
    "- X and y can be dataframes or of the type series (it all depends on the model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _____defining target and features____\n",
    "\n",
    "# columns_to_drop = ['price', 'number_of_rooms', 'terrace_area', 'garden_area', 'furnished', 'garden', 'terrace', 'number_of_facades', 'locality_name', 'main_city', 'province']\n",
    "columns_to_drop = ['price', 'locality_name', 'main_city']\n",
    "\n",
    "# Drop the specified columns\n",
    "X = df.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "y = df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _____creating training and testing sets____\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "\n",
    "- replacing missing values with statistical estimates of these missing data of these missing data or with arbitrary values\n",
    "- it's common to use the mean or median to fill in missing values\n",
    "- for categorical values we can use the mode (most frequent value)\n",
    "- !!! must split our data first, to avoid data leakage !!!\n",
    "- flag as missing or -1 or -999\n",
    "\n",
    "https://medium.com/mastering-the-art-of-data-science-a-comprehensive/mastering-data-preprocessing-and-feature-engineering-for-machine-learning-missing-basic-data-a648d2cb1196#:~:text=Missing%20Data%20Imputation%3A%20Imputation%20is,models%20or%20obtain%20their%20predictions.\n",
    "\n",
    "- These are the columns that have missing values in our case (for houses):\n",
    "\n",
    "    ```\n",
    "    number_of_rooms           8188\n",
    "    terrace_area              7969\n",
    "    swimming_pool             7550 --> 1/0 or NaN\n",
    "    garden_area               7511\n",
    "    furnished                 7404 --> 1/0 or NaN\n",
    "    garden                    6775 --> 1/0 or NaN\n",
    "    terrace                   4813 --> 1/0 or NaN\n",
    "    kitchen_type              4046 --> object      -----> One Hot Encoding\n",
    "    fully_equipped_kitchen    2422 --> 1/0 or NaN  -----> REMOVED (wrong logic)\n",
    "    state_of_building         2338 --> object      -----> One Hot Encoding\n",
    "    number_of_facades         1765\n",
    "    property_subtype          0                    -----> One Hot Encoding\n",
    "    ```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Simple Imputation \n",
    "\n",
    "- filling with mean value: `missing = lambda x: x.fillna(x.mean())`\n",
    "- or filling with median value: `missing = lambda x: x.fillna(x.median())`\n",
    "- then using transform(): `df = df.transform(missing)`\n",
    "\n",
    "- or do the same in another way:\n",
    "    ``` python\n",
    "    mean_value = df['column_name'].mean()\n",
    "    df['column_name'] = df['column_name'].fillna(mean_value, inplace=True)\n",
    "    ```\n",
    "- SimpleImputer\n",
    "\n",
    "    ```python\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    df[['column_name']] = mean_imputer.fit_transform(df[['column_name']])\n",
    "    \n",
    "    constant_imputer = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=-999)\n",
    "    df[['column_name']] = constant_imputer.fit_transform(df[['column_name']])\n",
    "\n",
    "    frequent_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    df[['column_name']] = frequent_imputer.fit_transform(df[['column_name']])\n",
    "\n",
    "    constant_imputer = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=MISS)\n",
    "    df[['column_name']] = constant_imputer.fit_transform(df[['column_name']])\n",
    "    ```\t\n",
    "\n",
    "### Advanced Imputation\n",
    "\n",
    "- ! important to first normalize the data !\n",
    "\n",
    "- K-nearest neighbors\n",
    "- SMOTE (synthetic minority over-sampling technique)\n",
    "\n",
    "    ``` Python\n",
    "    from sklearn.impute import KNNImputer\n",
    "    imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "    df['column_name'] = imputer.fit_transform(df['column_name'])\n",
    "    ```   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list only columns that have null values\n",
    "null_colums = df.columns[df.isnull().any()]\n",
    "print(df[null_colums].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _____imputing -1 for missing values for number_of_rooms, terrace_area, garden_area, furnished, garden, terrace, number_of_facades____\n",
    "\n",
    "constant_imputer = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=-1)\n",
    "\n",
    "colums_to_impute = ['number_of_rooms', 'terrace_area', 'garden_area', 'furnished', 'garden', 'terrace', 'number_of_facades']\n",
    "\n",
    "X_train[colums_to_impute] = constant_imputer.fit_transform(X_train[colums_to_impute])\n",
    "X_test[colums_to_impute] = constant_imputer.fit_transform(X_test[colums_to_impute])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list only columns that have null values\n",
    "null_colums = X_train.columns[X_train.isnull().any()]\n",
    "print(X_train[null_colums].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.metrics  \n",
    "import math\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "linmodel = LinearRegression()\n",
    "\n",
    "#____train the model____\n",
    "linmodel.fit(X_train, y_train)\n",
    "\n",
    "#____show the score____\n",
    "print(\"linmodel training score: \", linmodel.score(X_train, y_train))\n",
    "\n",
    "#____test the model____\n",
    "y_lin_prediction = linmodel.predict(X_test)\n",
    "\n",
    "#____show the score____\n",
    "print(\"linmodel testscore: \", linmodel.score(X_test, y_test))\n",
    "\n",
    "#____show the rmse____\n",
    "mse = sklearn.metrics.mean_squared_error(y_test, y_lin_prediction)\n",
    "rmse = math.sqrt(mse)\n",
    "print(\"rmse: \",rmse)\n",
    "\n",
    "#____show the r2_score____\n",
    "print(\"train_r2_score: \", r2_score(y_train, linmodel.predict(X_train)))\n",
    "print(\"test_r2_score: \", r2_score(y_test, linmodel.predict(X_test)))\n",
    "\n",
    "\n",
    "print(\"cross val score: \", cross_val_score(linmodel, X_train, y_train).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.subplots(figsize = (7, 7))\n",
    "\n",
    "plt.scatter(y_test, y_lin_prediction)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'y', lw=2)\n",
    "plt.xlabel(\"Real prices\", size=10)\n",
    "plt.ylabel(\"Predicted prices\", size=10)\n",
    "plt.title(\"Linear Regression\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression\n",
    "\n",
    "!! runs 5m34s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn.metrics  \n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [10, 15, 20, 25, 30, 40],\n",
    "    'n_estimators': [100, 200]}\n",
    "grid = GridSearchCV(RandomForestRegressor(), param_grid=param_grid)\n",
    "grid.fit(X, y)\n",
    "dept = grid.best_params_['max_depth']\n",
    "estimators = grid.best_params_['n_estimators']\n",
    "\n",
    "randomforest = RandomForestRegressor(n_estimators = estimators, max_depth = dept)\n",
    "\n",
    "#____train the model____\n",
    "randomforest.fit(X_train, y_train)\n",
    "\n",
    "#____show the choosen max_depth____\n",
    "print(\"randomforest choosen max_depth: \", dept)\n",
    "\n",
    "#____show the choosen n_estimators____\n",
    "print(\"randomforest choosen n_estimators: \", estimators)\n",
    "\n",
    "#____show the score____\n",
    "print(\"randomforest training score: \", randomforest.score(X_train, y_train))\n",
    "\n",
    "#____test the model____\n",
    "y_prediction = randomforest.predict(X_test)\n",
    "\n",
    "#____show the score____\n",
    "print(\"randomforest testscore: \", randomforest.score(X_test, y_test))\n",
    "\n",
    "mse = sklearn.metrics.mean_squared_error(y_test, y_prediction)\n",
    "rmse = math.sqrt(mse)\n",
    "print(\"rmse: \",rmse)\n",
    "\n",
    "print(\"cross val score: \", cross_val_score(randomforest, X_train, y_train).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.subplots(figsize = (7, 7))\n",
    "\n",
    "plt.scatter(y_test, y_prediction)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'y', lw=2)\n",
    "plt.xlabel(\"Real prices\", size=10)\n",
    "plt.ylabel(\"Predicted prices\", size=10)\n",
    "plt.title(\"Random Forest\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#feature importance\n",
    "importance_rf = pd.Series(randomforest.feature_importances_, index=X.columns)\n",
    "\n",
    "#sort importances\n",
    "sorted_importance_rf = importance_rf.sort_values()\n",
    "\n",
    "sorted_importance_rf.plot(kind='barh', color='lightgreen', figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "https://campus.datacamp.com/courses/designing-machine-learning-workflows-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('regressor', RandomForestRegressor())\n",
    "])\n",
    "\n",
    "params = dict(\n",
    "    regressor__max_depth = [10, 15, 20, 25, 30],\n",
    "    regressor__n_estimators = [100, 200]\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid=params)\n",
    "gs = grid_search.fit(X_train, y_train).best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "clf = RandomForestRegressor(max_depth = gs['regressor__max_depth'], n_estimators = gs['regressor__n_estimators']).fit(X_train, y_train)\n",
    "\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(clf, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load the model\n",
    "\n",
    "with open('model.pkl', 'rb') as file:\n",
    "    clf2 =  pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "norm = Normalizer()\n",
    "\n",
    "X_train_norm = norm.fit_transform(X_train)\n",
    "X_test_norm = norm.transform(X_test)\n",
    "\n",
    "#____train the model____\n",
    "linmodel.fit(X_train_norm, y_train)\n",
    "\n",
    "#____show the score____\n",
    "print(\"linmodel training score: \", linmodel.score(X_train_norm, y_train))\n",
    "\n",
    "#____test the model____\n",
    "y_prediction = linmodel.predict(X_test_norm)\n",
    "\n",
    "#____show the score____\n",
    "print(\"linmodel testscore: \", linmodel.score(X_test_norm, y_test))\n",
    "\n",
    "#____show the rmse____\n",
    "mse = sklearn.metrics.mean_squared_error(y_test, y_prediction)\n",
    "rmse = math.sqrt(mse)\n",
    "print(\"rmse: \",rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train_stzd = sc.fit_transform(X_train)\n",
    "X_test_stzd = sc.transform(X_test)\n",
    "\n",
    "#____train the model____\n",
    "linmodel.fit(X_train_stzd, y_train)\n",
    "\n",
    "#____show the score____\n",
    "print(\"linmodel training score: \", linmodel.score(X_train_stzd, y_train))\n",
    "\n",
    "#____test the model____\n",
    "y_prediction = linmodel.predict(X_test_stzd)\n",
    "\n",
    "#____show the score____\n",
    "print(\"linmodel testscore: \", linmodel.score(X_test_stzd, y_test))\n",
    "\n",
    "#____show the rmse____\n",
    "mse = sklearn.metrics.mean_squared_error(y_test, y_prediction)\n",
    "rmse = math.sqrt(mse)\n",
    "print(\"rmse: \",rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stepwise regression attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Initialize an RFE object\n",
    "rfe = RFECV(linmodel, cv=5)\n",
    "\n",
    "# Fit the RFE object to the data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Print the optimal number of features\n",
    "print(f\"Optimal number of features: {rfe.n_features_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SelectFromModel\n",
    "\n",
    "- SelectFromModel is a meta-transformer in scikit-learn that can be used to select features based on importance weights. \n",
    "- It works by using the importance weights provided by the underlying model to select features.\n",
    "- When you fit the SelectFromModel transformer to your data, it will use the coef_ or feature_importances_ attributes of the underlying model to determine which features to select. \n",
    "- You can specify a threshold to determine the importance level for feature selection.\n",
    "- The features returned by SelectFromModel are the subset of features that meet the specified importance threshold.\n",
    "\n",
    "\n",
    "\n",
    "#### After execution:\n",
    "```\n",
    "Index(['postal_code', 'number_of_rooms', 'living_area', 'furnished',\n",
    "       'terrace_area', 'garden_area', 'surface_of_good', 'number_of_facades'],\n",
    "      dtype='object')\n",
    "```\n",
    "Tested to keep only those as features, but result was much worse:\n",
    "\n",
    "```\n",
    "linmodel training score:  0.36424632346334607\n",
    "linmodel testscore:  0.33422335259159885\n",
    "rmse:  136370.33357315898\n",
    "train_r2_score:  0.36424632346334607\n",
    "test_r2_score:  0.33422335259159885\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['price', 'locality_name', 'main_city', 'province']\n",
    "\n",
    "# Drop the specified columns\n",
    "df = df.drop(columns=columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs = -1, class_weight = 'balanced', max_depth = 15)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "model = SelectFromModel(rf, prefit=True)\n",
    "essential_features = model.get_support()\n",
    "features = df.columns[essential_features]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "# Initialize a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Initialize an RFE object\n",
    "rfe = RFECV(model, cv=5)\n",
    "\n",
    "# Fit the RFE object to the data\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# Print the optimal number of features\n",
    "print(f\"Optimal number of features: {rfe.n_features_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://dataaspirant.com/stepwise-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = df\n",
    "\n",
    "columns_to_drop = ['price', 'locality_name', 'main_city', 'province']\n",
    "\n",
    "\n",
    "# Drop the specified columns\n",
    "X = df.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "y = df['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.5,random_state =123)\n",
    "clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n",
    "models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPEN TOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to drop for apartments\n",
    "df.surface_of_good.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of an array of just one record to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "my_array = np.array(X_train.iloc[0])\n",
    "my_array\n",
    "\n",
    "# to be able to use it for prediction you need to reshape it to a 2D array\n",
    "my_array = my_array.reshape(1, -1)\n",
    "\n",
    "probabilities = linmodel.predict_proba(my_array)\n",
    "prediction = linmodel.predict(my_array)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
